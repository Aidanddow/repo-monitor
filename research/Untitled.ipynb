{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06baaa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GitPython\n",
      "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.5/182.5 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from GitPython) (4.0.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, gitdb, GitPython\n",
      "Successfully installed GitPython-3.1.29 gitdb-4.0.9 smmap-5.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GitPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe81843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e0d1d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../src/backend/repos/cs20-main/\"\n",
    "repo = Repo(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad05f2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<git.remote.FetchInfo at 0x7fe0da6dbcb0>,\n",
       " <git.remote.FetchInfo at 0x7fe0da6dbd70>,\n",
       " <git.remote.FetchInfo at 0x7fe0da6db4d0>,\n",
       " <git.remote.FetchInfo at 0x7fe0da6db650>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.remotes.origin.fetch('+refs/heads/*:refs/remotes/origin/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b21c505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment\n",
      "developer\n",
      "json\n",
      "master\n"
     ]
    }
   ],
   "source": [
    " for branch in repo.remote().refs:\n",
    "        print(branch.remote_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fb302a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(repo.iter_commits(branch.remote_head))\n",
    "# for c in a:\n",
    "#     print(list(a))\n",
    "\n",
    "c1 = a[-2]\n",
    "c2 = a[-10]\n",
    "\n",
    "for file in list(c1.diff(c2)):\n",
    "    \n",
    "    try:\n",
    "        print(file.a_blob.data_stream.read().decode('utf-8'))\n",
    "        if file.b_blob:\n",
    "            print(\"-----------------------------------------------\")\n",
    "            print(file.b_blob.data_stream.read().decode('utf-8'))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0bb2b4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".gitignore\n",
      "=======================================================\n",
      "lhs: 100644 | b21e278303706391892a98e077f978d06f80ccae\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "__pycache__/\n",
      ".DS_Store\n",
      "\n",
      "# Django #\n",
      "*.log\n",
      "*.pot\n",
      "*.pyc\n",
      "__pycache__\n",
      "db.sqlite3\n",
      "media\n",
      "migrations\n",
      "\n",
      "# Backup files # \n",
      "*.bak \n",
      "\n",
      "# If you are using PyCharm # \n",
      "# User-specific stuff\n",
      ".idea/**/workspace.xml\n",
      ".idea/**/tasks.xml\n",
      ".idea/**/usage.statistics.xml\n",
      ".idea/**/dictionaries\n",
      ".idea/**/shelf\n",
      "\n",
      "# AWS User-specific\n",
      ".idea/**/aws.xml\n",
      "\n",
      "# Generated files\n",
      ".idea/**/contentModel.xml\n",
      "\n",
      "# Sensitive or high-churn files\n",
      ".idea/**/dataSources/\n",
      ".idea/**/dataSources.ids\n",
      ".idea/**/dataSources.local.xml\n",
      ".idea/**/sqlDataSources.xml\n",
      ".idea/**/dynamic.xml\n",
      ".idea/**/uiDesigner.xml\n",
      ".idea/**/dbnavigator.xml\n",
      "\n",
      "# Gradle\n",
      ".idea/**/gradle.xml\n",
      ".idea/**/libraries\n",
      "\n",
      "# File-based project format\n",
      "*.iws\n",
      "\n",
      "# IntelliJ\n",
      "out/\n",
      "\n",
      "# JIRA plugin\n",
      "atlassian-ide-plugin.xml\n",
      "\n",
      "# Python # \n",
      "*.py[cod] \n",
      "*$py.class \n",
      "\n",
      "# Distribution / packaging \n",
      ".Python build/ \n",
      "develop-eggs/ \n",
      "dist/ \n",
      "downloads/ \n",
      "eggs/ \n",
      ".eggs/ \n",
      "lib/ \n",
      "lib64/ \n",
      "parts/ \n",
      "sdist/ \n",
      "var/ \n",
      "wheels/ \n",
      "*.egg-info/ \n",
      ".installed.cfg \n",
      "*.egg \n",
      "*.manifest \n",
      "*.spec \n",
      "\n",
      "# Installer logs \n",
      "pip-log.txt \n",
      "pip-delete-this-directory.txt \n",
      "\n",
      "# Unit test / coverage reports \n",
      "htmlcov/ \n",
      ".tox/ \n",
      ".coverage \n",
      ".coverage.* \n",
      ".cache \n",
      ".pytest_cache/ \n",
      "nosetests.xml \n",
      "coverage.xml \n",
      "*.cover \n",
      ".hypothesis/ \n",
      "\n",
      "# Jupyter Notebook \n",
      ".ipynb_checkpoints \n",
      "\n",
      "# pyenv \n",
      ".python-version \n",
      "\n",
      "# celery \n",
      "celerybeat-schedule.* \n",
      "\n",
      "# SageMath parsed files \n",
      "*.sage.py \n",
      "\n",
      "# Environments \n",
      ".env \n",
      ".venv \n",
      "env/ \n",
      "venv/ \n",
      "ENV/ \n",
      "env.bak/ \n",
      "venv.bak/ \n",
      "\n",
      "# mkdocs documentation \n",
      "/site \n",
      "\n",
      "# mypy \n",
      ".mypy_cache/ \n",
      "\n",
      "# Sublime Text # \n",
      "*.tmlanguage.cache \n",
      "*.tmPreferences.cache \n",
      "*.stTheme.cache \n",
      "*.sublime-workspace \n",
      "*.sublime-project \n",
      "\n",
      "# sftp configuration file \n",
      "sftp-config.json \n",
      "\n",
      "# Package control specific files Package \n",
      "Control.last-run \n",
      "Control.ca-list \n",
      "Control.ca-bundle \n",
      "Control.system-ca-bundle \n",
      "GitHub.sublime-settings \n",
      "\n",
      "# Visual Studio Code # \n",
      ".vscode/* \n",
      "!.vscode/settings.json \n",
      "!.vscode/tasks.json \n",
      "!.vscode/launch.json \n",
      "!.vscode/extensions.json \n",
      ".history\n",
      "\n",
      "# Saved Folder\n",
      "backend/saved/\n",
      "\n",
      ".gitlab-ci.yml\n",
      "=======================================================\n",
      "lhs: 100644 | 247a8848926ef009510a9b76b7c4afc88052605f\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "image: python:3.9\n",
      "\n",
      "django_tests:\n",
      "\n",
      "    tags:\n",
      "        - docker\n",
      "    script:\n",
      "        - pip3 install -r backend/requirements.txt\n",
      "        - pip3 install opencv-python-headless  \n",
      "\n",
      "        - python3 backend/manage.py makemigrations\n",
      "        - python3 backend/manage.py migrate\n",
      "        - python3 backend/manage.py check\n",
      "\n",
      "        - python3 backend/manage.py test backend/\n",
      "    only:\n",
      "        - developer \n",
      "\n",
      ".gitlab/issue_templates/.gitkeep\n",
      "=======================================================\n",
      "lhs: 100644 | e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "\n",
      ".gitlab/issue_templates/Bug.md\n",
      "=======================================================\n",
      "lhs: 100644 | 8cd9bab55072ab9258a5ec26687c53472d2ff044\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "<!--\n",
      "Note: Please search to see if an issue already exists for the bug you encountered.\n",
      "-->\n",
      "\n",
      "#### Priority/Severity:\n",
      "\n",
      "- [ ] **High** (Impacts the normal user flow or blocks app usage)\n",
      "- [ ] **Medium** (Negatively affects the user experience)\n",
      "- [ ] **Low** (Typos, missing icons, layout issues, etc.)\n",
      "\n",
      "## Current Behavior:\n",
      "<!-- A concise description of what you're experiencing. -->\n",
      "\n",
      "### Steps to Reproduce\n",
      "<!--\n",
      "Example: steps to reproduce the behavior:\n",
      "1. In this environment...\n",
      "2. With this config...\n",
      "3. Run '...'\n",
      "4. See error...\n",
      "-->\n",
      "\n",
      "## Expected Behavior:\n",
      "<!-- A concise description of what you expected to happen. -->\n",
      "\n",
      "\n",
      "\n",
      "#### Anything else:\n",
      "<!--\n",
      "Links? References? Anything that will give us more context about the issue that you are encountering!\n",
      "-->\n",
      "\n",
      ".gitlab/issue_templates/Enhancement.md\n",
      "=======================================================\n",
      "lhs: 100644 | 32d5ee513136819dfa5495caa4ae6a38660afecf\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "<!--\n",
      "Note: Please search to see if a similar issue already exists\n",
      "-->\n",
      "\n",
      "#### Priority/Severity:\n",
      "\n",
      "- [ ] **High** (Huge increase in performance/productivity/usability/legislative cover)\n",
      "- [ ] **Medium** (Good increase in performance/productivity/usability)\n",
      "- [ ] **Low** (Trivial, minor improvements) \n",
      "\n",
      "## Current Behavior:\n",
      "<!-- A description of what you would like to change -->\n",
      "\n",
      "## Proposed Improvement:\n",
      "<!-- A description of how could it be improved -->\n",
      "\n",
      "\n",
      "### Anything else:\n",
      "\n",
      ".gitlab/issue_templates/Feature.md\n",
      "=======================================================\n",
      "lhs: 100644 | 37bfa78ad08d055afb0f549810f721d2f150cdfa\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "<!--\n",
      "Note: Please search to see if a similar issue already exists\n",
      "-->\n",
      "\n",
      "#### Priority/Severity:\n",
      "\n",
      "- [ ] **High** (Huge increase in performance/productivity/usability/legislative cover)\n",
      "- [ ] **Medium** (Good increase in performance/productivity/usability)\n",
      "- [ ] **Low** (Trivial, minor improvements) \n",
      "\n",
      "## Problem/Motivation:\n",
      "<!-- A description of what the implementation lacks -->\n",
      "\n",
      "## Proposed Solution:\n",
      "<!-- A description of what you would like to add -->\n",
      "\n",
      "\n",
      "### Anything else:\n",
      "\n",
      "LICENSE\n",
      "=======================================================\n",
      "lhs: 100644 | 39cfc2543749de6d5bef2f712ca98a0dc252e4a5\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "MIT License\n",
      "\n",
      "Copyright (c) 2022 Team Project H / 2021 / CS20\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n",
      "README.md\n",
      "=======================================================\n",
      "lhs: 100644 | 3f13a69e84106c29bee5f4771013eb47a5111064\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "# Streamline Tables - CS20 Team Project\n",
      "\n",
      "## Description\n",
      "Streamline Tables is a web extension for Google Chrome which simplifies the process of downloading tabular data from the web. It can process tables from\n",
      "- HTML\n",
      "- PDF's\n",
      "\n",
      "The project uses Javascript for the frontend, with a Django webapp as the backend.\n",
      "\n",
      "\n",
      "## Usage\n",
      "Right click on the desired table, and the data will be downloaded as a .xls file\n",
      "\n",
      "\n",
      "## Setup\n",
      "\n",
      "Load Chrome extension in Chrome by going to Settings->Extensions->Load Unpacked, then selecting the project extension folder.\\\n",
      "Running the Django app backend requires python version 3.8 or later, this can be found here - https://www.python.org/downloads/\n",
      "\n",
      "\n",
      "### Automatically - Windows \n",
      "\n",
      "The First time you download the file run the Install.bat file - this will install all the python packages for the backend.\\\n",
      "Then when you want to run the Streamline Table run the Start.bat file - While the terminal is running you will be able to use the extension freely. \n",
      "\n",
      "### Mac/Linux \n",
      "\n",
      "From the terminal, navigate to the cs20-main folder, then:\n",
      "```\n",
      "% cd backend\n",
      "% python -m venv venv\n",
      "% source venv/bin/activate\n",
      "% pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "### Manually \n",
      "Setup the django models from the backend folder with\n",
      "```\n",
      "% rm db.sqlite3\n",
      "% python manage.py makemigrations streamline\n",
      "% python manage.py migrate\n",
      "```\n",
      "Then run the django webapp with\n",
      "```\n",
      "% python manage.py runserver\n",
      "```\n",
      "\n",
      "## Run Instructions\n",
      "\n",
      "Download the folder extension,\n",
      "Go to chrome://extensions\n",
      "Enable Developer Mode in top right\n",
      "Select load unpacked in top left\n",
      "Navigate to folder and select\n",
      "Extension should now be installed\n",
      "\n",
      "NOTE: this does not need to be done each time you reopen chrome\\\n",
      "NOTE: Does not work unless server is running \n",
      "\n",
      "When updating the code for the extension, the extension automatically updates when you save your changes, no need to redo the steps above\n",
      "\n",
      "## Information on the Django backend - run with Start.bat files\n",
      "\n",
      "This only needs to be running on one machine on a wifi network and the extension will work on all other machines on the same wifi network.\\\n",
      "It could also be setup to be hosted by one machine and be made public so that any machine on any network can access it with having to run the Start file. More information here - https://docs.djangoproject.com/en/3.2/howto/deployment/\n",
      "\n",
      "\n",
      "Windows_Install.bat\n",
      "=======================================================\n",
      "lhs: 100644 | eaa03cc15dce1a35a5e1c0040859cccc8b28f0f6\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "start cmd /k \"cd backend && python -m venv venv && venv\\Scripts\\activate.bat && pip install -r requirements.txt\"\n",
      "Windows_Start.bat\n",
      "=======================================================\n",
      "lhs: 100644 | 95519ccebd91d39152d35d7d00ac11a674aa6ad8\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "start cmd /k \"cd backend && venv\\Scripts\\activate.bat && python manage.py runserver 127.0.0.1:8000\"\n",
      "\n",
      "backend/config/__init__.py\n",
      "=======================================================\n",
      "lhs: 100644 | e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "\n",
      "backend/config/settings.py\n",
      "=======================================================\n",
      "lhs: 100644 | f22e68ca29c145cd639e91f4fb50ff356d55b2dc\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "\"\"\"\n",
      "Django settings for Streamline Tables project.\n",
      "\n",
      "Generated by 'django-admin startproject' using Django 2.2.2.\n",
      "\n",
      "For more information on this file, see\n",
      "https://docs.djangoproject.com/en/2.2/topics/settings/\n",
      "\n",
      "For the full list of settings and their values, see\n",
      "https://docs.djangoproject.com/en/2.2/ref/settings/\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "\n",
      "# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\n",
      "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
      "\n",
      "SAVE_DIR = os.path.join(BASE_DIR, \"saved\")\n",
      "CSV_DIR = os.path.join(SAVE_DIR, \"csv\")\n",
      "PDF_DIR = os.path.join(SAVE_DIR, \"pdf\")\n",
      "\n",
      "if not os.path.exists(SAVE_DIR):\n",
      "    os.mkdir(SAVE_DIR)\n",
      "\n",
      "if not os.path.exists(PDF_DIR):\n",
      "    os.mkdir(PDF_DIR)\n",
      "\n",
      "if not os.path.exists(CSV_DIR):\n",
      "    os.mkdir(CSV_DIR)\n",
      "\n",
      "MAX_ENTRIES = 250\n",
      "\n",
      "# Time interval for the database cleaning task (in hours)\n",
      "CLEANING_INTERVAL = 4\n",
      "\n",
      "# Tuple time to live (in hours) - The time threshold after which a tuple can be deleted\n",
      "# For a CLEANING_INTERVAL = 4 and TUPLE_TTL = 2, a tuple could be deleted after a min of 4 hours and up to ~6 hours\n",
      "TUPLE_TTL = 2\n",
      "\n",
      "TEMPLATE_DIR = os.path.join(BASE_DIR, \"templates\")\n",
      "\n",
      "\n",
      "# Quick-start development settings - unsuitable for production\n",
      "# See https://docs.djangoproject.com/en/2.2/howto/deployment/checklist/\n",
      "\n",
      "# SECURITY WARNING: keep the secret key used in production secret!\n",
      "SECRET_KEY = \"-j9f9vc!@&zsu85sop*m2b_mrruf^9=mbta0%uxmy--n%oc@&6\"\n",
      "\n",
      "# SECURITY WARNING: don't run with debug turned on in production!\n",
      "DEBUG = True\n",
      "\n",
      "ALLOWED_HOSTS = []\n",
      "\n",
      "\n",
      "# Application definition\n",
      "\n",
      "INSTALLED_APPS = [\n",
      "    \"django.contrib.admin\",\n",
      "    \"django.contrib.auth\",\n",
      "    \"django.contrib.contenttypes\",\n",
      "    \"django.contrib.sessions\",\n",
      "    \"django.contrib.messages\",\n",
      "    \"django.contrib.staticfiles\",\n",
      "    \"streamline.apps.StreamlineConfig\",\n",
      "]\n",
      "\n",
      "MIDDLEWARE = [\n",
      "    \"django.middleware.security.SecurityMiddleware\",\n",
      "    \"django.contrib.sessions.middleware.SessionMiddleware\",\n",
      "    \"django.middleware.common.CommonMiddleware\",\n",
      "    \"django.middleware.csrf.CsrfViewMiddleware\",\n",
      "    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n",
      "    \"django.contrib.messages.middleware.MessageMiddleware\",\n",
      "    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n",
      "]\n",
      "\n",
      "ROOT_URLCONF = \"config.urls\"\n",
      "\n",
      "TEMPLATES = [\n",
      "    {\n",
      "        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n",
      "        \"DIRS\": [\n",
      "            TEMPLATE_DIR,\n",
      "        ],\n",
      "        \"APP_DIRS\": True,\n",
      "        \"OPTIONS\": {\n",
      "            \"context_processors\": [\n",
      "                \"django.template.context_processors.debug\",\n",
      "                \"django.template.context_processors.request\",\n",
      "                \"django.contrib.auth.context_processors.auth\",\n",
      "                \"django.contrib.messages.context_processors.messages\",\n",
      "            ],\n",
      "        },\n",
      "    },\n",
      "]\n",
      "\n",
      "WSGI_APPLICATION = \"config.wsgi.application\"\n",
      "\n",
      "\n",
      "# Database\n",
      "# https://docs.djangoproject.com/en/2.2/ref/settings/#databases\n",
      "\n",
      "DATABASES = {\n",
      "    \"default\": {\n",
      "        \"ENGINE\": \"django.db.backends.sqlite3\",\n",
      "        \"NAME\": os.path.join(BASE_DIR, \"db.sqlite3\"),\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "# Password validation\n",
      "# https://docs.djangoproject.com/en/2.2/ref/settings/#auth-password-validators\n",
      "\n",
      "AUTH_PASSWORD_VALIDATORS = [\n",
      "    {\n",
      "        \"NAME\": \"django.contrib.auth.password_validation.UserAttributeSimilarityValidator\",\n",
      "    },\n",
      "    {\n",
      "        \"NAME\": \"django.contrib.auth.password_validation.MinimumLengthValidator\",\n",
      "    },\n",
      "    {\n",
      "        \"NAME\": \"django.contrib.auth.password_validation.CommonPasswordValidator\",\n",
      "    },\n",
      "    {\n",
      "        \"NAME\": \"django.contrib.auth.password_validation.NumericPasswordValidator\",\n",
      "    },\n",
      "]\n",
      "\n",
      "\n",
      "# Internationalization\n",
      "# https://docs.djangoproject.com/en/2.2/topics/i18n/\n",
      "\n",
      "LANGUAGE_CODE = \"en-uk\"\n",
      "\n",
      "TIME_ZONE = \"UTC\"\n",
      "\n",
      "USE_I18N = True\n",
      "\n",
      "USE_L10N = True\n",
      "\n",
      "USE_TZ = True\n",
      "\n",
      "\n",
      "MEDIA_URL = \"/media/\"\n",
      "CSV_URL = \"/csv/\"\n",
      "STATIC_URL = \"/static/\"\n",
      "# STATICFILES_DIRS = (\n",
      "#     os.path.join(BASE_DIR, \"static/\"),\n",
      "# )\n",
      "\n",
      "backend/config/urls.py\n",
      "=======================================================\n",
      "lhs: 100644 | a1a475e77cad99a5fb370a0edadbc3173e8f452b\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "\"\"\"mysite URL Configuration\n",
      "\n",
      "The `urlpatterns` list routes URLs to views. For more information please see:\n",
      "    https://docs.djangoproject.com/en/2.2/topics/http/urls/\n",
      "Examples:\n",
      "Function views\n",
      "    1. Add an import:  from my_app import views\n",
      "    2. Add a URL to urlpatterns:  path('', views.home, name='home')\n",
      "Class-based views\n",
      "    1. Add an import:  from other_app.views import Home\n",
      "    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')\n",
      "Including another URLconf\n",
      "    1. Import the include() function: from django.urls import include, path\n",
      "    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))\n",
      "\"\"\"\n",
      "\n",
      "from django.contrib import admin\n",
      "from django.urls import include, path\n",
      "\n",
      "urlpatterns = [\n",
      "    path(\"streamline/\", include(\"streamline.urls\")),\n",
      "    path(\"admin/\", admin.site.urls),\n",
      "]\n",
      "\n",
      "backend/config/wsgi.py\n",
      "=======================================================\n",
      "lhs: 100644 | 5c51d2f15972c2497e0d4022fdad739b4d5a05a4\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "\"\"\"\n",
      "WSGI config for Streamline Tables project.\n",
      "\n",
      "It exposes the WSGI callable as a module-level variable named ``application``.\n",
      "\n",
      "For more information on this file, see\n",
      "https://docs.djangoproject.com/en/2.2/howto/deployment/wsgi/\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "\n",
      "from django.core.wsgi import get_wsgi_application\n",
      "\n",
      "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"config.settings\")\n",
      "application = get_wsgi_application()\n",
      "\n",
      "backend/manage.py\n",
      "=======================================================\n",
      "lhs: 100644 | 689b3f6196a6e372740e4c4685c98bd4a12488e9\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "#!/usr/bin/env python\n",
      "\"\"\"Django's command-line utility for administrative tasks.\"\"\"\n",
      "import os\n",
      "import sys\n",
      "\n",
      "\n",
      "def main():\n",
      "    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"config.settings\")\n",
      "    try:\n",
      "        from django.core.management import execute_from_command_line\n",
      "    except ImportError as exc:\n",
      "        raise ImportError(\n",
      "            \"Couldn't import Django. Are you sure it's installed and \"\n",
      "            \"available on your PYTHONPATH environment variable? Did you \"\n",
      "            \"forget to activate a virtual environment?\"\n",
      "        ) from exc\n",
      "    execute_from_command_line(sys.argv)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "backend/requirements.txt\n",
      "=======================================================\n",
      "lhs: 100644 | c0aa463f35c70a1913dad38023ce44a9294c4bc1\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "asgiref==3.5.0\n",
      "beautifulsoup4==4.10.0\n",
      "bs4==0.0.1\n",
      "camelot-py==0.10.1\n",
      "certifi==2021.10.8\n",
      "cffi==1.15.0\n",
      "chardet==4.0.0\n",
      "charset-normalizer==2.0.12\n",
      "click==8.0.3\n",
      "colorama==0.4.4\n",
      "cryptography==36.0.1\n",
      "Django==3.2\n",
      "et-xmlfile==1.1.0\n",
      "ghostscript==0.7\n",
      "idna==3.3\n",
      "lxml==4.7.1\n",
      "numpy==1.22.2\n",
      "opencv-python==4.5.5.62\n",
      "openpyxl==3.0.9\n",
      "pandas==1.4.1\n",
      "pdfminer.six==20211012\n",
      "pdftopng==0.2.3\n",
      "pycparser==2.21\n",
      "PyPDF2==1.26.0\n",
      "python-dateutil==2.8.2\n",
      "pytz==2021.3\n",
      "requests==2.27.1\n",
      "schedule==1.1.0\n",
      "six==1.16.0\n",
      "soupsieve==2.3.1\n",
      "sqlparse==0.4.2\n",
      "tabulate==0.8.9\n",
      "urllib3==1.26.8\n",
      "xlrd==2.0.1\n",
      "xlwt==1.3.0\n",
      "\n",
      "backend/streamline/__init__.py\n",
      "=======================================================\n",
      "lhs: 100644 | e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "\n",
      "backend/streamline/admin.py\n",
      "=======================================================\n",
      "lhs: 100644 | 8c38f3f3dad51e4585f3984282c2a4bec5349c1e\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "from django.contrib import admin\n",
      "\n",
      "# Register your models here.\n",
      "\n",
      "backend/streamline/apps.py\n",
      "=======================================================\n",
      "lhs: 100644 | 6d232c6f745c5a63f2074e24bc8ab98bf1e34eff\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "import os\n",
      "\n",
      "from django.apps import AppConfig\n",
      "from django.utils.module_loading import autodiscover_modules\n",
      "\n",
      "\n",
      "class StreamlineConfig(AppConfig):\n",
      "    name = \"streamline\"\n",
      "\n",
      "    def ready(self):\n",
      "        autodiscover_modules(\"signals\")\n",
      "\n",
      "        from . import db_cleaning\n",
      "\n",
      "        if os.environ.get(\"RUN_MAIN\", None) != \"true\":\n",
      "            db_cleaning.start_scheduler()\n",
      "\n",
      "\n",
      "default_app_config = \"streamline.apps.StreamlineConfig\"\n",
      "\n",
      "backend/streamline/db_cleaning.py\n",
      "=======================================================\n",
      "lhs: 100644 | 5cb14b7b6f90bddd706b575ef65404d398ca1698\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "# Code from https://stackoverflow.com/questions/44896618/django-run-a-function-every-x-seconds/60244694#60244694\n",
      "\n",
      "import threading\n",
      "import time\n",
      "from datetime import timedelta\n",
      "\n",
      "from django.conf import settings\n",
      "from django.utils.timezone import now\n",
      "from schedule import Scheduler\n",
      "\n",
      "from .models import Url_HTML, Url_PDF\n",
      "\n",
      "\n",
      "def remove_record(table, type):\n",
      "\n",
      "    # Code from @David Robinson at https://stackoverflow.com/questions/10345147/django-query-datetime-for-objects-older-than-5-hours\n",
      "\n",
      "    time_threshold = now() - timedelta(hours=settings.TUPLE_TTL)\n",
      "\n",
      "    results = table.objects.filter(created__lt=time_threshold)\n",
      "\n",
      "    if results:\n",
      "        for result in results:\n",
      "            result.delete()\n",
      "\n",
      "        print(\"---\", len(results), type, \"_TUPLE DELETED --- \")\n",
      "\n",
      "\n",
      "def clean_db():\n",
      "    print(\"--- Scheduled Database Cleaning\")\n",
      "\n",
      "    remove_record(Url_PDF, \"PDF\")\n",
      "    remove_record(Url_HTML, \"HTML\")\n",
      "\n",
      "\n",
      "def run_continuously(self, interval=1):\n",
      "    \"\"\"Continuously run, while executing pending jobs at each elapsed\n",
      "    time interval.\n",
      "    @return cease_continuous_run: threading.Event which can be set to\n",
      "    cease continuous run.\n",
      "    Please note that it is *intended behavior that run_continuously()\n",
      "    does not run missed jobs*. For example, if you've registered a job\n",
      "    that should run every minute and you set a continuous run interval\n",
      "    of one hour then your job won't be run 60 times at each interval but\n",
      "    only once.\n",
      "    \"\"\"\n",
      "\n",
      "    cease_continuous_run = threading.Event()\n",
      "\n",
      "    class ScheduleThread(threading.Thread):\n",
      "        @classmethod\n",
      "        def run(cls):\n",
      "            while not cease_continuous_run.is_set():\n",
      "                self.run_pending()\n",
      "                time.sleep(interval)\n",
      "\n",
      "    continuous_thread = ScheduleThread()\n",
      "    continuous_thread.setDaemon(True)\n",
      "    continuous_thread.start()\n",
      "    return cease_continuous_run\n",
      "\n",
      "\n",
      "Scheduler.run_continuously = run_continuously\n",
      "\n",
      "\n",
      "def start_scheduler():\n",
      "    print(\n",
      "        \"--- Cleaning Scheduler Running (every {n_hours} hours)\".format(\n",
      "            n_hours=settings.CLEANING_INTERVAL\n",
      "        )\n",
      "    )\n",
      "    scheduler = Scheduler()\n",
      "    scheduler.every(settings.CLEANING_INTERVAL).hours.do(clean_db)\n",
      "    scheduler.run_continuously()\n",
      "\n",
      "backend/streamline/forms.py\n",
      "=======================================================\n",
      "lhs: 100644 | e79048bfd8cc14e85800470f23f385407bc1f879\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "from django import forms\n",
      "\n",
      "backend/streamline/migrations/0001_initial.py\n",
      "=======================================================\n",
      "lhs: 100644 | 2a45a565572161bcfc565413318d0d96e05ba4a8\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "# Generated by Django 3.1.7 on 2022-02-22 17:28\n",
      "\n",
      "import django.db.models.deletion\n",
      "import django.utils.timezone\n",
      "from django.db import migrations, models\n",
      "\n",
      "\n",
      "class Migration(migrations.Migration):\n",
      "\n",
      "    initial = True\n",
      "\n",
      "    dependencies = [\n",
      "    ]\n",
      "\n",
      "    operations = [\n",
      "        migrations.CreateModel(\n",
      "            name='Url_HTML',\n",
      "            fields=[\n",
      "                ('id', models.AutoField(primary_key=True, serialize=False)),\n",
      "                ('url', models.CharField(max_length=200)),\n",
      "                ('doi', models.CharField(default='', max_length=200)),\n",
      "                ('created', models.DateTimeField(default=django.utils.timezone.now)),\n",
      "            ],\n",
      "        ),\n",
      "        migrations.CreateModel(\n",
      "            name='Url_PDF',\n",
      "            fields=[\n",
      "                ('id', models.AutoField(primary_key=True, serialize=False)),\n",
      "                ('url', models.CharField(max_length=200)),\n",
      "                ('pdf_path', models.FilePathField(default=None, null=True)),\n",
      "                ('created', models.DateTimeField(default=django.utils.timezone.now)),\n",
      "            ],\n",
      "        ),\n",
      "        migrations.CreateModel(\n",
      "            name='Table_PDF',\n",
      "            fields=[\n",
      "                ('id', models.AutoField(primary_key=True, serialize=False)),\n",
      "                ('page', models.IntegerField()),\n",
      "                ('file_path', models.FilePathField(default=None)),\n",
      "                ('pdf_id', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='streamline.url_pdf')),\n",
      "            ],\n",
      "        ),\n",
      "        migrations.CreateModel(\n",
      "            name='Table_HTML',\n",
      "            fields=[\n",
      "                ('id', models.AutoField(primary_key=True, serialize=False)),\n",
      "                ('file_path', models.FilePathField(default=None)),\n",
      "                ('html_id', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='streamline.url_html')),\n",
      "            ],\n",
      "        ),\n",
      "    ]\n",
      "\n",
      "backend/streamline/migrations/__init__.py\n",
      "=======================================================\n",
      "lhs: 100644 | e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "\n",
      "backend/streamline/models.py\n",
      "=======================================================\n",
      "lhs: 100644 | 9abcb5af62ee9541adaf2f1258a4c9944f569625\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "from django.conf import settings\n",
      "from django.db import models\n",
      "from django.utils.timezone import now\n",
      "\n",
      "\n",
      "class Url_PDF(models.Model):\n",
      "    id = models.AutoField(primary_key=True)\n",
      "    url = models.CharField(max_length=200)\n",
      "    pdf_path = models.FilePathField(null=True, default=None)\n",
      "    created = models.DateTimeField(default=now)\n",
      "\n",
      "    def save(self, *args, **kwargs):\n",
      "        if Url_PDF.objects.count() >= settings.MAX_ENTRIES:\n",
      "            Url_PDF.objects.earliest(\"id\").delete()\n",
      "            print(\"--- PDF_TUPLE DELETED --- \")\n",
      "\n",
      "        super(Url_PDF, self).save(*args, **kwargs)\n",
      "\n",
      "\n",
      "class Url_HTML(models.Model):\n",
      "    id = models.AutoField(primary_key=True)\n",
      "    url = models.CharField(max_length=200)\n",
      "    doi = models.CharField(max_length=200, default=\"\")\n",
      "    created = models.DateTimeField(default=now)\n",
      "\n",
      "    def save(self, *args, **kwargs):\n",
      "        if Url_HTML.objects.count() >= settings.MAX_ENTRIES:\n",
      "            Url_HTML.objects.earliest(\"id\").delete()\n",
      "            print(\"--- HTML_TUPLE DELETED --- \")\n",
      "\n",
      "        super(Url_HTML, self).save(*args, **kwargs)\n",
      "\n",
      "\n",
      "class Table_HTML(models.Model):\n",
      "    id = models.AutoField(primary_key=True)\n",
      "    html_id = models.ForeignKey(Url_HTML, on_delete=models.CASCADE)\n",
      "    file_path = models.FilePathField(default=None)\n",
      "\n",
      "\n",
      "class Table_PDF(models.Model):\n",
      "    id = models.AutoField(primary_key=True)\n",
      "    pdf_id = models.ForeignKey(Url_PDF, on_delete=models.CASCADE)\n",
      "    page = models.IntegerField()\n",
      "    file_path = models.FilePathField(default=None)\n",
      "\n",
      "backend/streamline/signals.py\n",
      "=======================================================\n",
      "lhs: 100644 | 39ad767772b0f8b2514a57985cc106d73b07008d\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "# code\n",
      "import os\n",
      "\n",
      "from django.conf import settings\n",
      "from django.db.models.signals import post_delete\n",
      "from django.dispatch import receiver\n",
      "\n",
      "from .models import Table_HTML, Table_PDF, Url_PDF\n",
      "\n",
      "PDF_PATH = settings.PDF_DIR\n",
      "CSV_PATH = settings.CSV_DIR\n",
      "\n",
      "\n",
      "@receiver(post_delete, sender=Url_PDF)\n",
      "def delete_pdf(sender, instance, **kwargs):\n",
      "\n",
      "    if os.path.isfile(instance.pdf_path):\n",
      "\n",
      "        os.chdir(PDF_PATH)\n",
      "        print(\"--- PDF\", instance.pdf_path, \"DELETED --- \")\n",
      "        os.remove(instance.pdf_path)\n",
      "\n",
      "\n",
      "@receiver(post_delete, sender=Table_PDF)\n",
      "def delete_pdf_csv(sender, instance, **kwargs):\n",
      "\n",
      "    if os.path.isfile(instance.file_path):\n",
      "\n",
      "        os.chdir(CSV_PATH)\n",
      "        print(\"--- CSV\", instance.file_path, \"DELETED --- \")\n",
      "        os.remove(instance.file_path)\n",
      "\n",
      "\n",
      "@receiver(post_delete, sender=Table_HTML)\n",
      "def delete_html_csv(sender, instance, **kwargs):\n",
      "\n",
      "    if os.path.isfile(instance.file_path):\n",
      "\n",
      "        os.chdir(CSV_PATH)\n",
      "        print(\"--- CSV\", instance.file_path, \"DELETED --- \")\n",
      "        os.remove(instance.file_path)\n",
      "\n",
      "backend/streamline/tests/__init__.py\n",
      "=======================================================\n",
      "lhs: 100644 | e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "\n",
      "backend/streamline/tests/test_models.py\n",
      "=======================================================\n",
      "lhs: 100644 | f15ab90d9f646c592006bc66237e6f020028c91d\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "from django.conf import settings\n",
      "from django.test import TestCase\n",
      "from streamline.models import Table_HTML, Table_PDF, Url_HTML, Url_PDF\n",
      "\n",
      "\n",
      "class TestUrlPdf(TestCase):\n",
      "    def test_url_pdf_creation(self):\n",
      "        pdf = Url_PDF.objects.create(url=\"test.pdf\", pdf_path=\"path/to/test.pdf\")\n",
      "\n",
      "        self.assertIsNotNone(pdf)\n",
      "\n",
      "    def test_url_pdf_deletion_if_max_entries_is_reached(self):\n",
      "\n",
      "        first_pdf = Url_PDF.objects.create(url=\"test.pdf\", pdf_path=\"path/to/test.pdf\")\n",
      "\n",
      "        for i in range(settings.MAX_ENTRIES):\n",
      "            Url_PDF.objects.create(\n",
      "                url=\"test_{i}.pdf\".format(i=i), pdf_path=\"path/to/test.pdf\"\n",
      "            )\n",
      "\n",
      "        result = Url_PDF.objects.filter(id=first_pdf.id).first()\n",
      "\n",
      "        self.assertIsNone(result)\n",
      "\n",
      "\n",
      "class TestUrlHtml(TestCase):\n",
      "    def test_url_html_creation(self):\n",
      "        html = Url_HTML.objects.create(url=\"test.com\")\n",
      "\n",
      "        self.assertIsNotNone(html)\n",
      "\n",
      "    def test_url_html_deletion_if_max_entries_is_reached(self):\n",
      "\n",
      "        first_html = Url_HTML.objects.create(\n",
      "            url=\"test.com\",\n",
      "        )\n",
      "\n",
      "        for i in range(settings.MAX_ENTRIES):\n",
      "            Url_HTML.objects.create(\n",
      "                url=\"test_{i}.com\".format(i=i),\n",
      "            )\n",
      "\n",
      "        result = Url_HTML.objects.filter(id=first_html.id).first()\n",
      "\n",
      "        self.assertIsNone(result)\n",
      "\n",
      "\n",
      "class TestTablePdf(TestCase):\n",
      "    def setUp(self) -> None:\n",
      "\n",
      "        self.pdf = Url_PDF.objects.create(url=\"test.pdf\", pdf_path=\"path/to/test.pdf\")\n",
      "\n",
      "        return super().setUp()\n",
      "\n",
      "    def test_table_pdf_creation(self):\n",
      "\n",
      "        table = Table_PDF.objects.create(\n",
      "            pdf_id=self.pdf, page=5, file_path=\"path/to/table.csv\"\n",
      "        )\n",
      "\n",
      "        self.assertIsNotNone(table)\n",
      "\n",
      "    def test_table_pdf_on_deletion_cascade(self):\n",
      "\n",
      "        table = Table_PDF.objects.create(\n",
      "            pdf_id=self.pdf, page=5, file_path=\"path/to/table.csv\"\n",
      "        )\n",
      "\n",
      "        self.assertIsNotNone(table)\n",
      "\n",
      "        Url_PDF.objects.filter(id=self.pdf.id).first().delete()\n",
      "\n",
      "        table = Table_PDF.objects.filter(id=table.id).first()\n",
      "\n",
      "        self.assertIsNone(table)\n",
      "\n",
      "\n",
      "class TestTableHtml(TestCase):\n",
      "    def setUp(self) -> None:\n",
      "\n",
      "        self.html = Url_HTML.objects.create(url=\"test.com\")\n",
      "\n",
      "        return super().setUp()\n",
      "\n",
      "    def test_table_html_creation(self):\n",
      "\n",
      "        table = Table_HTML.objects.create(\n",
      "            html_id=self.html, file_path=\"path/to/table.csv\"\n",
      "        )\n",
      "\n",
      "        self.assertIsNotNone(table)\n",
      "\n",
      "    def test_table_html_on_deletion_cascade(self):\n",
      "\n",
      "        table = Table_HTML.objects.create(\n",
      "            html_id=self.html, file_path=\"path/to/table.csv\"\n",
      "        )\n",
      "\n",
      "        self.assertIsNotNone(table)\n",
      "\n",
      "        Url_HTML.objects.filter(id=self.html.id).first().delete()\n",
      "\n",
      "        table = Table_HTML.objects.filter(id=table.id).first()\n",
      "\n",
      "        self.assertIsNone(table)\n",
      "\n",
      "backend/streamline/tests/test_urls.py\n",
      "=======================================================\n",
      "lhs: 100644 | ad7fd73969c84a5f741663f0abf66990a63cdd17\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "from django.test import SimpleTestCase\n",
      "from django.urls import resolve, reverse\n",
      "from streamline.views import (download_file, get_tables_from_html,\n",
      "                              get_tables_from_pdf)\n",
      "\n",
      "\n",
      "class TestUrls(SimpleTestCase):\n",
      "    def test_get_tables_from_pdf_is_resolved(self):\n",
      "        url = reverse(\"get_tables_from_pdf\")\n",
      "        self.assertEquals(resolve(url).func, get_tables_from_pdf)\n",
      "\n",
      "    def test_get_tables_from_html_is_resolved(self):\n",
      "        url = reverse(\"get_tables_from_html\")\n",
      "        self.assertEquals(resolve(url).func, get_tables_from_html)\n",
      "\n",
      "    def test_download_file_is_resolved(self):\n",
      "        url = reverse(\"download_file\", args=[\"1,2,3\", \"pdf\"])\n",
      "        self.assertEquals(resolve(url).func, download_file)\n",
      "\n",
      "backend/streamline/tests/test_views.py\n",
      "=======================================================\n",
      "lhs: 100644 | 18546de095e7c76fcb4a2256095cf3bb564941ce\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "from django.test import Client, TestCase\n",
      "from django.urls import reverse\n",
      "from streamline.models import Table_HTML, Table_PDF, Url_HTML, Url_PDF\n",
      "\n",
      "\n",
      "class TestTablesFromPdf(TestCase):\n",
      "    def setUp(self) -> None:\n",
      "\n",
      "        self.client = Client()\n",
      "        self.get_tables_from_pdf_url = reverse(\"get_tables_from_pdf\")\n",
      "\n",
      "        self.pdf_obj = Url_PDF.objects.create(\n",
      "            url=\"test.pdf\", pdf_path=\"path/to/test.pdf\"\n",
      "        )\n",
      "\n",
      "        Table_PDF.objects.create(\n",
      "            pdf_id=self.pdf_obj, page=\"6\", file_path=\"path/to/table.csv\"\n",
      "        )\n",
      "\n",
      "        return super().setUp()\n",
      "\n",
      "    def test_get_tables_from_pdf_GET(self):\n",
      "\n",
      "        response = self.client.get(\n",
      "            self.get_tables_from_pdf_url, {\"url\": \"test.pdf\", \"pages\": \"6\"}\n",
      "        )\n",
      "\n",
      "        self.assertEquals(response.status_code, 200)\n",
      "        self.assertTemplateUsed(response, \"streamline/preview_page.html\")\n",
      "\n",
      "    def test_get_tables_from_pdf_no_tables(self):\n",
      "\n",
      "        response = self.client.get(\n",
      "            self.get_tables_from_pdf_url, {\"url\": \"test.pdf\", \"pages\": \"1\"}\n",
      "        )\n",
      "\n",
      "        self.assertEquals(response.status_code, 200)\n",
      "        self.assertTemplateUsed(response, \"streamline/no_tables.html\")\n",
      "\n",
      "    def test_get_tables_from_pdf_invalid_request(self):\n",
      "\n",
      "        response = self.client.get(self.get_tables_from_pdf_url)\n",
      "\n",
      "        self.assertEquals(response.status_code, 400)\n",
      "\n",
      "    def test_get_tables_from_pdf_invalid_input(self):\n",
      "\n",
      "        response = self.client.get(\n",
      "            self.get_tables_from_pdf_url, {\"url\": \"test.pdf\", \"pages\": \"abc\"}\n",
      "        )\n",
      "\n",
      "        self.assertEquals(response.status_code, 400)\n",
      "\n",
      "\n",
      "class TestTablesFromHtml(TestCase):\n",
      "    def setUp(self) -> None:\n",
      "\n",
      "        self.client = Client()\n",
      "\n",
      "        self.get_tables_from_html_url = reverse(\"get_tables_from_html\")\n",
      "\n",
      "        self.html_obj = Url_HTML.objects.create(\n",
      "            url=\"test.com\",\n",
      "        )\n",
      "\n",
      "        Table_HTML.objects.create(html_id=self.html_obj, file_path=\"path/to/table.csv\")\n",
      "\n",
      "        self.html_obj_2 = Url_HTML.objects.create(\n",
      "            url=\"test2.com\",\n",
      "        )\n",
      "\n",
      "        return super().setUp()\n",
      "\n",
      "    def test_get_tables_from_html_GET(self):\n",
      "\n",
      "        response = self.client.get(self.get_tables_from_html_url, {\"url\": \"test.com\"})\n",
      "\n",
      "        self.assertEquals(response.status_code, 200)\n",
      "        self.assertTemplateUsed(response, \"streamline/preview_page.html\")\n",
      "\n",
      "    def test_get_tables_from_html_no_tables(self):\n",
      "\n",
      "        response = self.client.get(self.get_tables_from_html_url, {\"url\": \"test2.com\"})\n",
      "\n",
      "        self.assertEquals(response.status_code, 200)\n",
      "        self.assertTemplateUsed(response, \"streamline/no_tables.html\")\n",
      "\n",
      "    def test_get_tables_from_html_invalid_request(self):\n",
      "\n",
      "        response = self.client.get(self.get_tables_from_html_url)\n",
      "\n",
      "        self.assertEquals(response.status_code, 400)\n",
      "\n",
      "\n",
      "class TestDownloadFile(TestCase):\n",
      "    def setUp(self) -> None:\n",
      "\n",
      "        self.client = Client()\n",
      "\n",
      "        self.pdf_obj = Url_PDF.objects.create(\n",
      "            url=\"test.pdf\", pdf_path=\"path/to/test.pdf\"\n",
      "        )\n",
      "\n",
      "        self.table_obj = Table_PDF.objects.create(\n",
      "            pdf_id=self.pdf_obj, page=\"6\", file_path=\"path/to/table.csv\"\n",
      "        )\n",
      "\n",
      "        return super().setUp()\n",
      "\n",
      "    def test_download_file_not_found(self):\n",
      "\n",
      "        response = self.client.get(reverse(\"download_file\", args=[\"1\", \"html\"]))\n",
      "\n",
      "        self.assertEquals(response.status_code, 404)\n",
      "\n",
      "    def test_download_file_not_downloadable(self):\n",
      "\n",
      "        response = self.client.get(\n",
      "            reverse(\"download_file\", args=[self.table_obj.id, \"pdf\"])\n",
      "        )\n",
      "\n",
      "        self.assertEquals(response.status_code, 404)\n",
      "\n",
      "backend/streamline/urls.py\n",
      "=======================================================\n",
      "lhs: 100644 | 908541f24b0d3672aec48b307d3ad304ebf74a50\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "from django.conf.urls import url\n",
      "from django.urls import path\n",
      "\n",
      "from . import views\n",
      "\n",
      "urlpatterns = [\n",
      "    url(r\"get_page_data_HTML/$\", views.get_tables_from_html, name=\"get_tables_from_html\"),\n",
      "    url(r\"get_page_data_pdf/$\", views.get_tables_from_pdf, name=\"get_tables_from_pdf\"),\n",
      "    path(\"download_page/<str:table_ids>/<str:table_type>\", views.download_file, name=\"download_file\"),\n",
      "]\n",
      "\n",
      "backend/streamline/utils/generics.py\n",
      "=======================================================\n",
      "lhs: 100644 | 6a4b2a0a05c79560bedaf3c989ddbb87b65e06a4\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "import mimetypes\n",
      "import os\n",
      "import re\n",
      "from zipfile import ZipFile\n",
      "\n",
      "import pandas as pd\n",
      "from django.conf import settings\n",
      "from django.http import (HttpResponse, HttpResponseBadRequest,\n",
      "                         HttpResponseNotFound)\n",
      "from streamline.models import Table_HTML, Table_PDF\n",
      "\n",
      "\n",
      "def get_options(options_str):\n",
      "    \"\"\"\n",
      "    Receives a string of 1's and 0's corresponding to different user settings\n",
      "    1 = True, 0 = False\n",
      "\n",
      "    options_list[0]: \"Enable Footnotes\"\n",
      "    options_list[1]: \"Force Reprocess\"\n",
      "    \"\"\"\n",
      "\n",
      "    options_list = [True if char == \"1\" else False for char in options_str]\n",
      "\n",
      "    options = {\"enable_footnotes\": options_list[0], \"force_reprocess\": options_list[1]}\n",
      "\n",
      "    return options\n",
      "\n",
      "\n",
      "def create_zip(paths, folder=settings.CSV_DIR, zipPath=\"tables.zip\"):\n",
      "    \"\"\"\n",
      "    Will create and return the path to a zip file of all csv files in folder\n",
      "    \"\"\"\n",
      "    os.chdir(folder)\n",
      "    with ZipFile(zipPath, \"w\") as zipFile:\n",
      "        for path in paths:\n",
      "            zipFile.write(os.path.basename(path))\n",
      "\n",
      "    return os.path.abspath(zipPath)\n",
      "\n",
      "\n",
      "def check_valid_page_input(pages):\n",
      "    \"\"\"\n",
      "    Ensures the input follows a valid format\n",
      "    \"\"\"\n",
      "    regex = \"^all$|^\\s*[0-9]+\\s*((\\,|\\-)\\s*[0-9]+)*\\s*$\"\n",
      "    return re.search(regex, pages)\n",
      "\n",
      "\n",
      "def extract_doi(text):\n",
      "    \"\"\"\n",
      "    Should extract the doi if it is present from either a url, or html body\n",
      "\n",
      "    Regex found at: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
      "    https://stackoverflow.com/questions/27910/finding-a-doi-in-a-document-or-page\n",
      "    \"\"\"\n",
      "    doi_regex = \"\\b(10[.][0-9]{4,}(?:[.][0-9]+)*/\\S+)\"\n",
      "\n",
      "    if groups := re.search(doi_regex, text):\n",
      "        doi = groups.group(1)\n",
      "    else:\n",
      "        doi = \"\"\n",
      "\n",
      "    print(f\"doi = {doi}\" if doi else \"DOI not found\")\n",
      "    return doi.replace(\"/\", \"_\")\n",
      "\n",
      "\n",
      "def get_filepaths_from_id(table_ids, table_type):\n",
      "    \"\"\"\n",
      "    Takes a list of table_ids and returns a list of each tables corresponding csv path\n",
      "    \"\"\"\n",
      "    model = Table_PDF if table_type == \"pdf\" else Table_HTML\n",
      "\n",
      "    table_list = [\n",
      "        table_obj.file_path\n",
      "        for id in table_ids.split(\",\")\n",
      "        if (table_obj := model.objects.filter(id=int(id)).first())\n",
      "    ]\n",
      "\n",
      "    return table_list\n",
      "\n",
      "\n",
      "def get_as_html(table):\n",
      "    \"\"\"\n",
      "    Returns a html representation of a table, or an error message if table couldn't be read\n",
      "    \"\"\"\n",
      "    try:\n",
      "\n",
      "        if table.file_path.endswith(\".csv\"):\n",
      "            df_xls = pd.read_csv(\n",
      "                table.file_path, index_col=False, skip_blank_lines=True\n",
      "            )\n",
      "        else:\n",
      "            df_xls = pd.read_excel(table.file_path, index_col=False)\n",
      "\n",
      "        df_xls.dropna(how=\"all\", inplace=True)\n",
      "        df_xls.fillna(\"\", inplace=True)\n",
      "\n",
      "        df_xls.set_index(df_xls.columns[0], inplace=True)\n",
      "\n",
      "        df_xls.columns = [\"\" if \"Unnamed:\" in c else c for c in df_xls.columns]\n",
      "\n",
      "        csv_html = df_xls.to_html(\n",
      "            classes=\"table table-sm table-hover table-responsive\", border=0\n",
      "        )\n",
      "\n",
      "    except:\n",
      "        csv_html = \"<p>Preview not available</p>\"\n",
      "\n",
      "    return csv_html\n",
      "\n",
      "\n",
      "def create_context(url_obj, tables_obj, table_type=\"pdf\"):\n",
      "    \"\"\"\n",
      "    Creates and returns a dictionary providing context to the webpage\n",
      "    \"\"\"\n",
      "    table_ids = \",\".join([str(table.id) for table in tables_obj])\n",
      "    tables_html = []\n",
      "    if table_type == \"pdf\":\n",
      "        tables_html = [\n",
      "            (str(i + 1), get_as_html(tables_obj[i]), str(tables_obj[i].page))\n",
      "            for i in range(len(tables_obj))\n",
      "        ]\n",
      "    else:\n",
      "        tables_html = [\n",
      "            (str(i + 1), get_as_html(tables_obj[i])) for i in range(len(tables_obj))\n",
      "        ]\n",
      "\n",
      "    context_dict = {\n",
      "        \"url_id\": url_obj.id,\n",
      "        \"table_count\": len(tables_obj),\n",
      "        \"table_type\": table_type,\n",
      "        \"Web_Page_Tables\": tables_html,\n",
      "        \"table_ids\": table_ids,\n",
      "    }\n",
      "\n",
      "    return context_dict\n",
      "\n",
      "\n",
      "def get_data_from_request(request, get_pages=False):\n",
      "    \"\"\"\n",
      "    Takes a request, retrieves the url, options and optionally the pages\n",
      "    from the request. Handles errors in request.\n",
      "    This data is then printed and returned.\n",
      "    \"\"\"\n",
      "    url = request.GET.get(\"url\", None)\n",
      "    options = request.GET.get(\"options\", None)\n",
      "    pages = request.GET.get(\"pages\", None)\n",
      "\n",
      "    if not url:\n",
      "        print(\"\\n--- No URL found\")\n",
      "        return HttpResponseBadRequest(\"<h1>Invalid Request</h1>\")\n",
      "\n",
      "    if get_pages and not pages:\n",
      "        print(\"--- No Pages found\")\n",
      "        return HttpResponseBadRequest(\"<h1>Invalid Request</h1>\")\n",
      "\n",
      "    options_dict = {}\n",
      "    if options:\n",
      "        options_dict = get_options(options)\n",
      "\n",
      "    print(f\"\\n--- Url: {url}\")\n",
      "    print(f\"--- Options: {options_dict}\")\n",
      "    print(f\"--- Pages: {pages}\" if get_pages else \"\")\n",
      "\n",
      "    return url, options_dict, pages\n",
      "\n",
      "\n",
      "def create_file_response(file_path):\n",
      "    \"\"\"\n",
      "    Creates a HttpResponse with a file attatched, and returns the response\n",
      "    \"\"\"\n",
      "    if not os.path.isfile(file_path):\n",
      "        print(\"--- File(s) cannot be downloaded\")\n",
      "        return HttpResponseNotFound(\"<h1>File(s) cannot be downloaded</h1>\")\n",
      "\n",
      "    print(f\"--- Sending file {file_path} as HttpResponse\")\n",
      "\n",
      "    # The function guess_type() returns a tuple (type, encoding) we disregard the encoding\n",
      "    mime_type, _ = mimetypes.guess_type(file_path)\n",
      "\n",
      "    # Get the name of the file, disregarding the file path\n",
      "    fname = os.path.basename(file_path)\n",
      "\n",
      "    with open(file_path, \"rb\") as file:\n",
      "        response = HttpResponse(file, content_type=mime_type)\n",
      "        response[\"Content-Disposition\"] = f\"attachment; filename={fname}\"\n",
      "        return response\n",
      "\n",
      "backend/streamline/utils/html_to_csv.py\n",
      "=======================================================\n",
      "lhs: 100644 | 2c7f22c4e6e355a49025d0e03590fb93d8cc571e\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "import os\n",
      "\n",
      "import requests\n",
      "import xlwt\n",
      "from bs4 import BeautifulSoup\n",
      "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
      "from streamline.models import Table_HTML\n",
      "\n",
      "from . import generics\n",
      "\n",
      "\n",
      "def extract(url, web_page, options, save_path=None):\n",
      "    \"\"\"\n",
      "    Takes a url, finds all html tables and processes them,\n",
      "    saving their data to xls files.\n",
      "    \"\"\"\n",
      "    print(f\"--- Reading {url}\")\n",
      "\n",
      "    header = {\"User-Agent\": \"Mozilla/5.0\"}\n",
      "    session = requests.session()\n",
      "\n",
      "    try:\n",
      "        html = session.get(url, headers=header)\n",
      "        html.raise_for_status()\n",
      "    except requests.exceptions.ConnectionError as e:\n",
      "        print(e)\n",
      "        # Disable InsecureRequestWarning\n",
      "        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
      "        # close SSL verify to solve SSL error. This gives a InsecureRequestWarning\n",
      "        html = session.get(url, headers=header, verify=False)\n",
      "    except requests.exceptions.HTTPError as e:\n",
      "        print(e)\n",
      "        return 0\n",
      "\n",
      "    soup = BeautifulSoup(html.text, \"lxml\")\n",
      "\n",
      "    # Get titles, and footnotes of tables\n",
      "    titleList = [title.text for title in soup.select('header[class*=\"table\"]')]\n",
      "\n",
      "    footnotes = [footnote for footnote in soup.select('div[class*=\"footnote\"]')]\n",
      "    footnoteList = list()\n",
      "\n",
      "    if options[\"enable_footnotes\"]:\n",
      "        footnoteList = process_footnote(footnotes)\n",
      "\n",
      "    # Get doi from url. If not found, try to find in the rest of page\n",
      "    if not (doi := generics.extract_doi(url)):\n",
      "        doi = generics.extract_doi(soup.text)\n",
      "\n",
      "    web_page.doi = doi\n",
      "\n",
      "    tableList = soup.find_all(\"table\")\n",
      "\n",
      "    for num, table in enumerate(tableList):\n",
      "        print(f\"--- Processing Table {num+1}\")\n",
      "        tableArray, formattedData = process_table(table)\n",
      "\n",
      "        footnoteData = footnoteList[num] if len(footnoteList) > num else None\n",
      "\n",
      "        # If a title exists for this table, pass it\n",
      "        title = None\n",
      "\n",
      "        if len(titleList) > num:\n",
      "            title = titleList[num] if \"Table\" in titleList[num] else None\n",
      "\n",
      "        write_to_csv(\n",
      "            tableArray,\n",
      "            formattedData,\n",
      "            footnoteData,\n",
      "            num,\n",
      "            web_page,\n",
      "            title=title,\n",
      "            path=save_path,\n",
      "        )\n",
      "\n",
      "    print(\"--- Finished Processing Tables!\")\n",
      "\n",
      "    # Give number of tables to views to create ids in database for each one\n",
      "    return len(tableList)\n",
      "\n",
      "\n",
      "def process_footnote(footnotes):\n",
      "    \"\"\"\n",
      "    Returns a list with a list of <li> items from each tables footnotes\n",
      "    \"\"\"\n",
      "    # Remove '\\n' and more than one space\n",
      "    return [[\" \".join(li.text.split()) for li in fn.find_all(\"li\")] for fn in footnotes]\n",
      "\n",
      "\n",
      "def process_table(table):\n",
      "    \"\"\"\n",
      "    Takes a html table element and generates an array corresponding to the row and column data\n",
      "    \"\"\"\n",
      "    dataList, formattedDataList = list(), list()\n",
      "    # headers = table.find_all(\"th\")\n",
      "\n",
      "    # loop through each tables thead tag\n",
      "    for thead in (theadNodes := table.find_all(\"thead\")):\n",
      "\n",
      "        # loop through each row in the tables headings\n",
      "        for row in thead.find_all(\"tr\"):\n",
      "            # add space for each row for the row headings on the left side of the table\n",
      "            tds = [\"\"]\n",
      "\n",
      "            # loop through each column heading for this row\n",
      "            for column in row.find_all(\"th\"):\n",
      "\n",
      "                if \"\\n\" in (data := column.text):\n",
      "                    data = data.replace(\"\\n\", \" \")\n",
      "\n",
      "                if data:\n",
      "                    tds.append(data)\n",
      "\n",
      "                # offest if the colspan is greater than 0\n",
      "                colspan = column.attrs.get(\"colspan\", 0)\n",
      "                if colspan != 0 and colspan.isdigit():\n",
      "                    for i in range(int(colspan) - 1):\n",
      "                        tds.append(\"\")\n",
      "\n",
      "            if len(tds) != 0:\n",
      "                dataList.append(tds)\n",
      "\n",
      "    # Get data from table\n",
      "    for tr in (trNodes := table.find_all(\"tr\")):\n",
      "        tds = list()\n",
      "\n",
      "        for td in tr.find_all(\"td\"):\n",
      "            # Remove link tag\n",
      "            if link := td.find(\"a\"):\n",
      "                link.extract()\n",
      "\n",
      "            if sup := td.find(\"sup\"):\n",
      "                sup.extract()\n",
      "\n",
      "            # Get the text for each cell, replacing empty strings with a dash\n",
      "            data = \"-\" if td.text == \"\" else \" \".join(td.text.split())\n",
      "\n",
      "            tds.append(data)\n",
      "\n",
      "        if len(tds) != 0:\n",
      "            dataList.append(tds)\n",
      "\n",
      "    # Get data in bold and italics format\n",
      "    boldNodes1 = table.find_all(\"strong\")\n",
      "    boldNodes2 = table.find_all(\"b\")\n",
      "    italicsNode = table.find_all(\"i\")\n",
      "    italicsList = [i.text for i in italicsNode]\n",
      "    boldList = [bold.text for bold in boldNodes1]\n",
      "    boldList += [bold.text for bold in boldNodes2]\n",
      "\n",
      "    formattedDataList.append(boldList)\n",
      "    formattedDataList.append(italicsList)\n",
      "\n",
      "    return dataList, formattedDataList\n",
      "\n",
      "\n",
      "def write_to_csv(\n",
      "    table, formattedData, footnoteData, num, web_page, title=None, path=None\n",
      "):\n",
      "    \"\"\"\n",
      "    Takes a 2D array and writes the data to an xls file in the Desktop\n",
      "    \"\"\"\n",
      "    wbk = xlwt.Workbook()\n",
      "    sheet = wbk.add_sheet(\"Sheet1\", cell_overwrite_ok=True)\n",
      "    # Count the last row\n",
      "    line = 0\n",
      "    # font and style\n",
      "    style = xlwt.XFStyle()\n",
      "    font = xlwt.Font()\n",
      "\n",
      "    # Write title into excel\n",
      "    if title:\n",
      "        sheet.write(0, 0, title)\n",
      "\n",
      "    # Loop through arrays and write data into xls sheet\n",
      "    for row_index, row in enumerate(table):\n",
      "        for col_index, data in enumerate(row):\n",
      "            if data in formattedData[0]:\n",
      "                font.bold = True\n",
      "                style.font = font\n",
      "                sheet.write(row_index + 1, col_index, data, style=style)\n",
      "            elif data in formattedData[1]:\n",
      "                font.italic = True\n",
      "                style.font = font\n",
      "                sheet.write(row_index + 1, col_index, data, style=style)\n",
      "            else:\n",
      "                sheet.write(row_index + 1, col_index, data)\n",
      "        line = row_index\n",
      "\n",
      "    if footnoteData:\n",
      "        # Add \"FootNote\" title\n",
      "        line += 3\n",
      "        sheet.write(line, 0, \"FootNote\")\n",
      "        # Go to the next row\n",
      "        line += 1\n",
      "        # Write footnotes(surronding texts) into file\n",
      "        for footnote in footnoteData:\n",
      "            sheet.write(line, 0, footnote)\n",
      "            line += 1\n",
      "\n",
      "    # Save the file to \"path/{num}.xls\"\n",
      "    fname = f\"table{web_page.id}_{num+1}_{web_page.doi}.xls\"\n",
      "\n",
      "    wbk.save(path := os.path.join(path, fname))\n",
      "\n",
      "    # Creates a new table entry every time a new file is saved\n",
      "    Table_HTML.objects.create(html_id=web_page, file_path=path)\n",
      "\n",
      "    print(f\"--- Saved table {num+1} to {path}\")\n",
      "\n",
      "backend/streamline/utils/pdf_to_csv.py\n",
      "=======================================================\n",
      "lhs: 100644 | 88a855adc747bd310586d66c9ca2f02416e8116b\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "import os\n",
      "import urllib.request\n",
      "import uuid\n",
      "\n",
      "import camelot\n",
      "from streamline.models import Table_PDF\n",
      "\n",
      "HEADERS = {\n",
      "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36\"\n",
      "}\n",
      "\n",
      "\n",
      "def download_pdf(url, save_path=None):\n",
      "    \"\"\"\n",
      "    Simple script to download a pdf from url\n",
      "    \"\"\"\n",
      "    req = urllib.request.Request(url, headers=HEADERS)\n",
      "    response = urllib.request.urlopen(req)\n",
      "\n",
      "    # Generates name of pdf file\n",
      "    pdf_name = str(uuid.uuid4()) + \".pdf\"\n",
      "\n",
      "    path = os.path.join(save_path, pdf_name)\n",
      "\n",
      "    print(\"--- Downloading pdf\")\n",
      "\n",
      "    with open(path, \"wb\") as file:\n",
      "        file.write(response.read())\n",
      "\n",
      "    return path\n",
      "\n",
      "\n",
      "def download_pdf_tables(pdf_path, pdf_obj, save_path=None, pages=\"all\"):\n",
      "    print(\"--- Checking for tables in page\", pages)\n",
      "    print(\"--- PDF PATH ---\", pdf_path)\n",
      "\n",
      "    new_tables = []\n",
      "\n",
      "    try:\n",
      "        tables = camelot.read_pdf(pdf_path, pages=pages, flavor=\"stream\", edge_tol=100)\n",
      "    except Exception as e:\n",
      "\n",
      "        print(e, \"--- PDF is not valid\")\n",
      "        return new_tables\n",
      "\n",
      "    if len(tables) > 0:\n",
      "\n",
      "        # saves files with custom name\n",
      "        for i in range(len(tables)):\n",
      "            path = os.path.join(\n",
      "                save_path, f\"table{pdf_obj.id}_{tables[i].page}_{i}.csv\"\n",
      "            )\n",
      "            tables[i].to_csv(path)\n",
      "\n",
      "            # store each table from page\n",
      "            new_table = Table_PDF.objects.create(\n",
      "                pdf_id=pdf_obj, page=tables[i].page, file_path=path\n",
      "            )\n",
      "            new_tables.append(new_table)\n",
      "\n",
      "        print(f\"--- {len(tables)} CSV files saved to {save_path}\")\n",
      "    else:\n",
      "        print(\"--- No tables found\")\n",
      "\n",
      "    return new_tables\n",
      "\n",
      "\n",
      "def pages_to_int(pages):\n",
      "    \"\"\"\n",
      "    Transform the user input (a string) into a list of integers, each representing a page\n",
      "    Returns \"all\" if \"all\" is given\n",
      "    \"\"\"\n",
      "    if pages == \"all\":\n",
      "        return pages\n",
      "\n",
      "    page_list = []\n",
      "    for page in pages.split(\",\"):\n",
      "        if \"-\" in page:\n",
      "            page = page.split(\"-\")\n",
      "            page_range = list(range(int(page[0]), int(page[1]) + 1))\n",
      "            page_list += page_range\n",
      "        else:\n",
      "            page_list.append(page)\n",
      "\n",
      "    return page_list\n",
      "\n",
      "\n",
      "def get_missing_pages(page_list, pdf_obj_id, tables_obj):\n",
      "    \"\"\"\n",
      "    Check which tables are in the DB and return them as a list\n",
      "    Also, returns the missing pages\n",
      "    \"\"\"\n",
      "    if page_list == \"all\":\n",
      "        query = Table_PDF.objects.filter(pdf_id=pdf_obj_id)\n",
      "\n",
      "        print(f\"--- Found {len(query)} tables\")\n",
      "\n",
      "        for table in query:\n",
      "            tables_obj.append(table)\n",
      "\n",
      "        print(\"--- Missing pages: None\")\n",
      "\n",
      "        return \"\", tables_obj\n",
      "\n",
      "    pages = []\n",
      "\n",
      "    for page in page_list:\n",
      "\n",
      "        if not (query := Table_PDF.objects.filter(pdf_id=pdf_obj_id, page=page)):\n",
      "            pages.append(str(page))\n",
      "        else:\n",
      "            tables_obj += [table for table in query]\n",
      "\n",
      "    missing_pages = \",\".join(pages)\n",
      "    print(\"--- Missing pages:\", missing_pages or \"None\")\n",
      "\n",
      "    return missing_pages, tables_obj\n",
      "\n",
      "backend/streamline/views.py\n",
      "=======================================================\n",
      "lhs: 100644 | 324373817354cbd508846ffbdceee94e83e17923\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "from django.conf import settings\n",
      "from django.http import HttpResponseBadRequest, HttpResponseNotFound\n",
      "from django.shortcuts import render\n",
      "from streamline.models import Table_HTML, Url_HTML, Url_PDF\n",
      "\n",
      "from .utils import generics, html_to_csv, pdf_to_csv\n",
      "\n",
      "# Path to which resulting csv files will be saved (will be .../cs20-main/backend/saved)\n",
      "CSV_PATH = settings.CSV_DIR\n",
      "PDF_PATH = settings.PDF_DIR\n",
      "\n",
      "\n",
      "def get_tables_from_html(request):\n",
      "    \"\"\"\n",
      "    Extracts table data from HTML\n",
      "    \"\"\"\n",
      "    request_data = generics.get_data_from_request(request, get_pages=False)\n",
      "\n",
      "    if type(request_data) == HttpResponseBadRequest:\n",
      "        return request_data\n",
      "\n",
      "    url, options, _ = request_data\n",
      "\n",
      "    # Try to find tables in database\n",
      "    html_obj = Url_HTML.objects.filter(url=url).first()\n",
      "\n",
      "    # If not in database or reprocess is off\n",
      "    if not html_obj and not \"force_reprocess\" in options.keys():\n",
      "        # store URL\n",
      "        html_obj = Url_HTML.objects.create(url=url)\n",
      "        print(\"--- New HTML URL:\", html_obj.url)\n",
      "\n",
      "        # process page\n",
      "        html_to_csv.extract(url, html_obj, options, save_path=CSV_PATH)\n",
      "\n",
      "    # Query extracted tables\n",
      "\n",
      "    if tables_obj := Table_HTML.objects.filter(html_id=html_obj.id):\n",
      "        context_dict = generics.create_context(html_obj, tables_obj, table_type=\"html\")\n",
      "        return render(request, \"streamline/preview_page.html\", context=context_dict)\n",
      "    else:\n",
      "        return render(request, \"streamline/no_tables.html\", context={})\n",
      "\n",
      "\n",
      "def get_tables_from_pdf(request):\n",
      "    \"\"\"\n",
      "    Extracts table data from PDF\n",
      "    \"\"\"\n",
      "    request_data = generics.get_data_from_request(request, get_pages=True)\n",
      "\n",
      "    if type(request_data) == HttpResponseBadRequest:\n",
      "        return request_data\n",
      "\n",
      "    url, options, pages = request_data\n",
      "    tables_obj = list()\n",
      "\n",
      "    # Check if page input is valid\n",
      "    if generics.check_valid_page_input(pages):\n",
      "        page_list = pdf_to_csv.pages_to_int(pages)\n",
      "\n",
      "        pdf_obj = Url_PDF.objects.filter(url=url).first()\n",
      "\n",
      "        # If the pdf already exists in db or force reprocess is off\n",
      "        if pdf_obj and not \"force_reprocess\" in options.keys():\n",
      "            print(\"--- PDF Found ---\", pdf_obj.url)\n",
      "            pdf_path = pdf_obj.pdf_path\n",
      "\n",
      "            pages, tables_obj = pdf_to_csv.get_missing_pages(\n",
      "                page_list, pdf_obj.id, tables_obj\n",
      "            )\n",
      "\n",
      "        else:\n",
      "            print(\"--- New PDF:\", url)\n",
      "            # downloads pdf from right click\n",
      "            pdf_path = pdf_to_csv.download_pdf(url, save_path=PDF_PATH)\n",
      "            # store URL\n",
      "            pdf_obj = Url_PDF.objects.create(url=url, pdf_path=pdf_path)\n",
      "\n",
      "        # convert its table(s) into csv(s) and get table count\n",
      "        if pages:\n",
      "            new_tables = pdf_to_csv.download_pdf_tables(\n",
      "                pdf_path, pdf_obj, save_path=CSV_PATH, pages=pages\n",
      "            )\n",
      "            tables_obj += new_tables\n",
      "\n",
      "    else:\n",
      "        print(\"\\n--- Invalid page input\")\n",
      "        return HttpResponseBadRequest(\"<h1>Invalid Input</h1>\")\n",
      "\n",
      "    if tables_obj:\n",
      "        context_dict = generics.create_context(pdf_obj, tables_obj, table_type=\"pdf\")\n",
      "        return render(request, \"streamline/preview_page.html\", context=context_dict)\n",
      "\n",
      "    else:\n",
      "        return render(request, \"streamline/no_tables.html\", context={})\n",
      "\n",
      "\n",
      "def download_file(request, table_ids, table_type):\n",
      "    \"\"\"\n",
      "    A view to download either a zip of all tables, or a singular table\n",
      "    table_id of 0 indicates the user wants all tables\n",
      "    \"\"\"\n",
      "    if not table_ids or not table_type:\n",
      "        print(\"\\n--- Invalid Download request\")\n",
      "        return HttpResponseBadRequest(\"<h1>Invalid Request</h1>\")\n",
      "\n",
      "    # No table paths are required\n",
      "    if not (table_paths := generics.get_filepaths_from_id(table_ids, table_type)):\n",
      "        print(\"\\n--- File(s) not found\")\n",
      "        return HttpResponseNotFound(\"<h1>File(s) not found</h1>\")\n",
      "\n",
      "    # Only one table path is required -> send as file\n",
      "    elif len(table_paths) == 1:\n",
      "        file_path = table_paths[0]\n",
      "\n",
      "    # Multiple paths are required -> send as zip\n",
      "    else:\n",
      "        print(\"--- Sending tables as zip\")\n",
      "        file_path = generics.create_zip(table_paths, folder=CSV_PATH)\n",
      "\n",
      "    return generics.create_file_response(file_path)\n",
      "\n",
      "backend/templates/streamline/no_tables.html\n",
      "=======================================================\n",
      "lhs: 100644 | 3a8932e862340e9c3c8713075cbe488b268d38fb\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "<h1>No Tables found</h1>\n",
      "<p>It's probably that:</p>\n",
      "<ol>\n",
      "  <li>the website blocks scraping</li>\n",
      "  <li>tables are displayed as images</li>\n",
      "  <li>tables are inserted by links</li>\n",
      "</ol>\n",
      "<p><b>Try to download PDF and extract from PDF</b></p>\n",
      "\n",
      "backend/templates/streamline/preview_page.html\n",
      "=======================================================\n",
      "lhs: 100644 | fa27b7c9c7b6f8b278d97582b9419d07c6bb7366\n",
      "rhs: None\n",
      "file deleted in rhs\n",
      "{% load static %}\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n",
      "    <title>Extracted Tables</title>\n",
      "\n",
      "    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" integrity=\"sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3\" crossorigin=\"anonymous\">\n",
      "    <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js\" integrity=\"sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13\" crossorigin=\"anonymous\"></script>\n",
      "    \n",
      "</head>\n",
      "<body>\n",
      "        <div>\n",
      "                <nav class=\"navbar navbar-light justify-content-between bg-light\">\n",
      "                        <span class=\"navbar-brand mx-4 mb-0 h1\">Found {{ table_count }} tables</span>\n",
      "                        <ul class=\"navbar-nav\">\n",
      "                                <li class=\"nav-item\">\n",
      "                                        <a class=\"btn btn-outline-primary mx-4 mt-2\" href=\"{% url 'download_file' table_ids table_type %}\">Download All</a></p>\n",
      "                                </li>\n",
      "                        </ul>\n",
      "                </nav>\n",
      "        \n",
      "                <ul class=\"list-group list-group-flush\">\n",
      "                        {% for table in Web_Page_Tables %}\n",
      "                                <li class=\"list-group-item p-4\">\n",
      "                                        <div class=\"d-flex flex-row align-items-center justify-content-center\">\n",
      "                                                <h4 class=\"mb-0\">Table {{ table.0 }} - page {{table.2}}</h4>\n",
      "                                                <a class=\"btn btn-outline-primary mb-0 mx-4\" href=\"{% url 'download_file'  table.0 table_type %}\">Download</a>\n",
      "                                        </div>\n",
      "                                        <div class=\"p-4\">\n",
      "                                                {{table.1 | safe}}\n",
      "                                        </div>\n",
      "                                </li>\t\t\t\n",
      "                        {% endfor %}\n",
      "                </ul>\n",
      "        </div>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "\n",
      "extension/Steamline Tables-logos-crop.jpeg\n",
      "=======================================================\n",
      "lhs: 100644 | 5ee0d2bd0b4d168cef8937ace0e02a0ce07141a6\n",
      "rhs: None\n",
      "file deleted in rhs\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ln/18zrhm_n1b79bm28bhrknswm0000gn/T/ipykernel_10672/2696097717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhcommit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_blob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_blob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "hcommit = repo.head.commit\n",
    "# for file in hcommit.diff():\n",
    "#     print(file.a_blob.data_stream.read().decode(\"utf-8\"))\n",
    "    \n",
    "for file in hcommit.diff():\n",
    "    print(file)\n",
    "    print(file.a_blob.data_stream.read().decode('utf-8'))\n",
    "    if file.b_blob:\n",
    "        print(\"-----------------------------------------------\")\n",
    "        print(file.b_blob.data_stream.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5cc0ee3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..c219a5e0f98be124a4189cd4ffd477b8edc06264'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_branch = \"main\"\n",
    "commit_diff_string ='..' + repo.rev_parse('origin/' +\n",
    "                                      \"master\").hexsha\n",
    "commit_diff_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "72d07114",
   "metadata": {},
   "outputs": [
    {
     "ename": "GitCommandError",
     "evalue": "Cmd('git') failed due to: exit code(128)\n  cmdline: git diff HEAD~1\n  stderr: 'fatal: this operation must be run in a work tree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGitCommandError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ln/18zrhm_n1b79bm28bhrknswm0000gn/T/ipykernel_10672/2538263365.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HEAD~1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/git/cmd.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLazyMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_persistent_git_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/git/cmd.py\u001b[0m in \u001b[0;36m_call_process\u001b[0;34m(self, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexec_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_object_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/git/cmd.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, istream, with_extended_output, with_exceptions, as_process, output_stream, stdout_as_string, kill_after_timeout, with_stdout, universal_newlines, shell, env, max_chunk_size, strip_newline_in_stdout, **subprocess_kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_exceptions\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGitCommandError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredacted_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstdout_as_string\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# could also be output_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGitCommandError\u001b[0m: Cmd('git') failed due to: exit code(128)\n  cmdline: git diff HEAD~1\n  stderr: 'fatal: this operation must be run in a work tree'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "repo.git.diff('HEAD~1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ddba61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
